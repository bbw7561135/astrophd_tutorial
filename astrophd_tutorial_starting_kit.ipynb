{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.datascience-paris-saclay.fr\">\n",
    "<img src=\"img/logoUPSayPlusCDS_990.png\" width=\"600px\">\n",
    "</a>\n",
    "\n",
    "# Deep learning course for astro PhD students\n",
    "\n",
    "##### Alexandre Boucaud (Paris Saclay CDS)\n",
    "##### Marc Huertas-Company (LERMA)\n",
    "##### Bertrand Rigaud (CCIN2P3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [Data](#Data)\n",
    "3. [Workflow](#Workflow)\n",
    "4. [Evaluation](#Evaluation)\n",
    "5. [Local testing/exploration](#Local-testing)\n",
    "6. [Submission](#Submitting-to-ramp.studio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In astronomical images, the projection effects may cause two or more galaxies to overlap. When they are barely indistinguishable from one another, they are referred to as _blended_ and this can bias astrophysical estimators such as the morphology of galaxies or the shear (weak gravitational lensing distortion).  \n",
    "As the sensitivity of imaging devices grows, a high fraction of galaxies appear _blended_ in the images, which is a known and important issue for current and upcoming galaxy surveys.  \n",
    "\n",
    "In order not to discard such a wealth of information, it is key to develop methods to enable astronomers to alleviate such effect.\n",
    "We can foresee some features that would help, in which machine learning could provide a solution:\n",
    "- classify an image as containing isolated/blended objects  \n",
    "  ___binary classification___\n",
    "- count the number of blended sources in a blended image  \n",
    "  ___regression / object detection___\n",
    "- find the contours of each object  \n",
    "  ___object detection/segmentation___\n",
    "- ...\n",
    "\n",
    "In this exercice, we will approach the third item, the detection of contours, but in a constrained way : the images will only contain **two galaxies** and the goal will be to find the **contours of the overlapping region** between both galaxies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/data_test_mini.npy')\n",
    "y = np.load('data/labels_test_mini.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_basic(img, seg):\n",
    "    if seg.ndim == 3:\n",
    "        seg = seg.squeeze()\n",
    "    if img.ndim == 3:\n",
    "        img = img.squeeze()v\n",
    "\n",
    "    fig_size = (8, 4)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2,\n",
    "                           sharex=True, sharey=True, figsize=fig_size)\n",
    "    ax[0].imshow(img, origin='lower')\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('blended galaxies')\n",
    "    ax[1].imshow(seg, origin='lower',cmap='Greys_r')\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('segmap of overlaping region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "plot_data_basic(X[index], y[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the RAMP workflow is to help the participant focus on building optimized models to achieve a specific goal, by taking care of all other steps in the processing of the data. This can be summarized for this specific challenge with the following schematic\n",
    "\n",
    "<img src=\"img/workflow.svg\" width=\"150%\">\n",
    "\n",
    "The loading of the data, the actual training and testing and the scoring are taken care of behind the scenes, while the only input expected from participants is the ML model written as a _submission_ in a file called `object_detector.py`.\n",
    "\n",
    "This file will be part of a submission `<submission_name>` that should be placed under the `submissions` directory such as\n",
    "```\n",
    "submissions/<submission_name>/object_detector.py\n",
    "```\n",
    "\n",
    "To get you started, we have already written two different models.\n",
    "- `submissions/starting_kit/object_detector.py`\n",
    "- `submissions/keras_fcnn/object_detector.py`\n",
    "\n",
    "Inside `object_detector.py`, the workflow expects a Python class called `ObjectDetector` \n",
    "with two required methods implemented : `fit()` and `predict()`\n",
    "\n",
    "```python\n",
    "class ObjectDetector:\n",
    "    def __init__(self): pass\n",
    "    def fit(self): pass\n",
    "    def predict(self): pass\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy model - `starting-kit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load submissions/starting_kit/object_detector.py\n",
    "import numpy as np\n",
    "\n",
    "class ObjectDetector:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.zeros_like(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example deep net - `keras_fcnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load submissions/keras_fcnn/object_detector.py\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.noise import GaussianNoise\n",
    "\n",
    "\n",
    "class ObjectDetector(object):\n",
    "    \"\"\"Object detector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int, optional\n",
    "        The batch size used during training. Set by default to 32 samples.\n",
    "\n",
    "    epoch : int, optional\n",
    "        The number of epoch for which the model will be trained. Set by default\n",
    "        to 50 epochs.\n",
    "\n",
    "    model_check_point : bool, optional\n",
    "        Whether to create a callback for intermediate models.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    model_ : object\n",
    "        The DNN model.\n",
    "\n",
    "    params_model_ : Bunch dictionary\n",
    "        All hyper-parameters to build the DNN model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size=32, epoch=10, model_check_point=True):\n",
    "        self.model_, self.params_model_ = self._build_model()\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.model_check_point = model_check_point\n",
    "\n",
    "    def fit(self, X, y, pretrained=False):\n",
    "\n",
    "        if pretrained:\n",
    "            # for showcase load weights (this is not possible\n",
    "            # for an actual submission)\n",
    "            self.model_.load_weights(\n",
    "                'submissions/keras_fcnn/fcnn_weights_best.h5')\n",
    "            return\n",
    "\n",
    "        # build the box encoder to later encode y to make usable in the model\n",
    "        train_dataset = BatchGeneratorBuilder(X, y)\n",
    "        train_generator, val_generator, n_train_samples, n_val_samples = \\\n",
    "            train_dataset.get_train_valid_generators(\n",
    "                batch_size=self.batch_size)\n",
    "\n",
    "        # create the callbacks to get during fitting\n",
    "        callbacks = self._build_callbacks()\n",
    "\n",
    "        # fit the model\n",
    "        self.model_.fit_generator(\n",
    "            generator=train_generator,\n",
    "            steps_per_epoch=ceil(n_train_samples / self.batch_size),\n",
    "            epochs=self.epoch,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=ceil(n_val_samples / self.batch_size))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model_.predict(np.expand_dims(X, -1))\n",
    "\n",
    "    ###########################################################################\n",
    "    # Setup model\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_params_model():\n",
    "        params_model = Bunch()\n",
    "\n",
    "        # image and class parameters\n",
    "        params_model.img_rows = 128\n",
    "        params_model.img_cols = 128\n",
    "        params_model.img_channels = 1\n",
    "\n",
    "        # architecture params\n",
    "        params_model.output_channels = 1            # size of the output in depth\n",
    "        params_model.depth = 16                     # depth of all hidden layers\n",
    "        params_model.n_layers = 6                   # number of layers before last\n",
    "        params_model.conv_size0 = (3, 3)            # kernel size of first layer\n",
    "        params_model.conv_size = (3, 3)             # kernel size of intermediate layers\n",
    "        params_model.last_conv_size = (3, 3)        # kernel size of last layer\n",
    "        params_model.activation = 'relu'            # activation between layers\n",
    "        params_model.last_activation = 'sigmoid'    # final activation (sigmoid nice if binary objective)\n",
    "        params_model.initialization = 'he_normal'   # weight initialization\n",
    "        params_model.constraint = None              # kernel constraints (None, nonneg, unitnorm, maxnorm)\n",
    "        params_model.dropout_rate = 0.0             # percentage of weights not updated (0 = no dropout)\n",
    "        params_model.sigma_noise = 0.01             # random noise added before last layer (0 = no noise added)\n",
    "\n",
    "        # optimizer parameters\n",
    "        params_model.lr = 1e-4\n",
    "        params_model.beta_1 = 0.9\n",
    "        params_model.beta_2 = 0.999\n",
    "        params_model.epsilon = 1e-08\n",
    "        params_model.decay = 5e-05\n",
    "\n",
    "        # loss parameters\n",
    "        params_model.keras_loss = 'binary_crossentropy'\n",
    "\n",
    "        # callbacks parameters\n",
    "        params_model.early_stopping = True\n",
    "        params_model.es_patience = 12\n",
    "        params_model.es_min_delta = 0.001\n",
    "\n",
    "        params_model.reduce_learning_rate = True\n",
    "        params_model.lr_patience = 5\n",
    "        params_model.lr_factor = 0.5\n",
    "        params_model.lr_min_delta = 0.001\n",
    "        params_model.lr_cooldown = 2\n",
    "\n",
    "        return params_model\n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        # load the parameter for the SSD model\n",
    "        params_model = self._init_params_model()\n",
    "\n",
    "        model = fcnn_model(\n",
    "            input_shape=(params_model.img_rows,\n",
    "                         params_model.img_cols,\n",
    "                         params_model.img_channels),\n",
    "            output_channels=params_model.output_channels,\n",
    "            depth=params_model.depth,\n",
    "            n_layers=params_model.n_layers,\n",
    "            conv_size0=params_model.conv_size0,\n",
    "            conv_size=params_model.conv_size,\n",
    "            last_conv_size=params_model.last_conv_size,\n",
    "            activation=params_model.activation,\n",
    "            last_activation=params_model.last_activation,\n",
    "            dropout_rate=params_model.dropout_rate,\n",
    "            sigma_noise=params_model.sigma_noise,\n",
    "            initialization=params_model.initialization,\n",
    "            constraint=params_model.constraint)\n",
    "\n",
    "        optimizer = Adam(lr=params_model.lr)\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss=params_model.keras_loss)\n",
    "\n",
    "        return model, params_model\n",
    "\n",
    "    def _build_callbacks(self):\n",
    "        callbacks = []\n",
    "\n",
    "        if self.model_check_point:\n",
    "            callbacks.append(\n",
    "                ModelCheckpoint('./fcnn_weights_best.h5',\n",
    "                                monitor='val_loss',\n",
    "                                save_best_only=True,\n",
    "                                save_weights_only=True,\n",
    "                                period=1,\n",
    "                                verbose=1))\n",
    "        # add early stopping\n",
    "        if self.params_model_.early_stopping:\n",
    "            callbacks.append(\n",
    "                EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=self.params_model_.es_min_delta,\n",
    "                              patience=self.params_model_.es_patience,\n",
    "                              verbose=1))\n",
    "\n",
    "        # reduce learning-rate when reaching plateau\n",
    "        if self.params_model_.reduce_learning_rate:\n",
    "            callbacks.append(\n",
    "                ReduceLROnPlateau(monitor='val_loss',\n",
    "                                  factor=self.params_model_.lr_factor,\n",
    "                                  patience=self.params_model_.lr_patience,\n",
    "                                  cooldown=self.params_model_.lr_cooldown,\n",
    "                                  # min_delta=self.params_model_.lr_min_delta,\n",
    "                                  verbose=1))\n",
    "\n",
    "        return callbacks\n",
    "\n",
    "\n",
    "def fcnn_model(input_shape, output_channels, depth, n_layers,\n",
    "               conv_size0, conv_size, last_conv_size,\n",
    "               activation, last_activation,\n",
    "               dropout_rate, sigma_noise,\n",
    "               initialization, constraint):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(depth, conv_size0,\n",
    "                     input_shape=input_shape,\n",
    "                     activation=activation,\n",
    "                     padding='same',\n",
    "                     name='conv0',\n",
    "                     kernel_initializer=initialization,\n",
    "                     kernel_constraint=constraint))\n",
    "    if dropout_rate > 0:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for layer_n in range(1, n_layers):\n",
    "        model.add(Conv2D(depth, conv_size,\n",
    "                         activation=activation,\n",
    "                         padding='same',\n",
    "                         name=\"conv{}\".format(layer_n),\n",
    "                         kernel_initializer=initialization,\n",
    "                         kernel_constraint=constraint))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "    if sigma_noise > 0:\n",
    "        model.add(GaussianNoise(sigma_noise))\n",
    "\n",
    "    model.add(Conv2D(output_channels, last_conv_size,\n",
    "                     activation=last_activation,\n",
    "                     padding='same',\n",
    "                     name='last',\n",
    "                     kernel_initializer=initialization,\n",
    "                     kernel_constraint=constraint))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For image detection a classical metric is the ***Intersection over Union (IoU)*** also referred to as ***Jaccard index*** and defined as\n",
    "\n",
    "$$ IoU(A, B) =  \\dfrac{|A \\cap B|}{|A \\cup B|} $$\n",
    "\n",
    "This metric is very sensitive to small shifts or area difference between truth and prediction, as can be seen on\n",
    "\n",
    "<img src=\"img/iou_examples.png\" width=\"70%\">\n",
    "\n",
    "Typically, a value of $ IoU > 0.5 $ is used to define a good detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation of the IoU for a series of flatten segmentation images $\\in [0, 1]$ is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred):\n",
    "    # epsilon value\n",
    "    EPS = np.finfo(float).eps\n",
    "    # convert array to boolean\n",
    "    y_true_b = y_true.astype(bool)\n",
    "    y_pred_b = y_pred.astype(bool)\n",
    "    # compute binary intersection & union\n",
    "    intersection = np.sum(y_true_b & y_pred_b, axis=-1)\n",
    "    union = np.sum(y_true_b | y_pred_b, axis=-1)\n",
    "    \n",
    "    return (intersection + EPS) / (union + EPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the metric that will be used for this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local testing\n",
    "\n",
    "The usual way to work with RAMP is to explore solutions, add feature transformations, select models, perhaps do some AutoML/hyperopt, etc., _locally_, and checking them with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ramp_test_submission --quick-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script prints mean cross-validation scores \n",
    "\n",
    "```\n",
    "Testing Astro PhD tutorial - galaxy deblending\n",
    "Reading train and test files from ./data ...\n",
    "Reading cv ...\n",
    "Training ./submissions/starting_kit ...\n",
    "CV fold 0\n",
    "\tscore    IoU\n",
    "\ttrain  0.091|\n",
    "\tvalid  0.029\n",
    "\ttest   0.020\n",
    "CV fold 1\n",
    "\tscore    IoU\n",
    "\ttrain  0.045\n",
    "\tvalid  0.121\n",
    "\ttest   0.020\n",
    "CV fold 2\n",
    "\tscore    IoU\n",
    "\ttrain  0.075\n",
    "\tvalid  0.061\n",
    "\ttest   0.020\n",
    "----------------------------\n",
    "Mean CV scores\n",
    "----------------------------\n",
    "\tscore            IoU\n",
    "\ttrain  0.07 ± 0.0191\n",
    "\tvalid  0.07 ± 0.0381\n",
    "\ttest      0.02 ± 0.0\n",
    "----------------------------\n",
    "Bagged scores\n",
    "----------------------------\n",
    "\tscore    IoU\n",
    "\tvalid  0.021\n",
    "\ttest   0.023\n",
    "```\n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. \n",
    "\n",
    "For this we provide a unit test. Note that the test runs on your files in [`submissions/starting_kit`](/tree/submissions/starting_kit), not on the classes defined in the cells of this notebook.\n",
    "\n",
    "***Check list:***\n",
    "\n",
    "- make sure `ramp-workflow` is installed locally (see [readme](README.md))\n",
    "- make sure the data has been downloaded (see [readme](README.md))\n",
    "\n",
    "- make sure a Python file `object_detector.py` is in the  `submissions/<your_submission>` folder\n",
    "\n",
    "Finally, make sure there are no obvious code errors and the local processing goes through by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ramp_test_submission --quick-test --submission <your_submission>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you want to perform the test on the full data set ***(only if downloaded)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ramp_test_submission --submission <your_submission>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to [ramp.studio](http://ramp.studio)\n",
    "\n",
    "Once you found a good model, you can submit them to [ramp.studio](http://www.ramp.studio). First, if it is your first time using RAMP, [sign up](http://www.ramp.studio/sign_up), otherwise [log in](http://www.ramp.studio/login). \n",
    "Then find the event related to this challenge: [astrophd_tutorial](http://www.ramp.studio/events/astrophd_tutorial) and sign up for the event. \n",
    "Both signups are controled by RAMP administrators, so there **can be a delay between asking for signup and being able to submit**.\n",
    "\n",
    "Once your signup request is accepted, you can go to your [sandbox](http://www.ramp.studio/events/astrophd_tutorial/sandbox) and copy-paste (or upload) [`object_detector.py`](/edit/submissions/starting_kit/object_detector.py). Save it, rename it, then submit it. The submission is trained and tested on our backend in the same way as `ramp_test_submission` does it locally. While your submission is waiting in the queue and being trained, you can find it in the \"New submissions (pending training)\" table in [my submissions](http://www.ramp.studio/events/astrophd_tutorial/my_submissions). Once it is trained, you get a mail, and your submission shows up on the [public leaderboard](http://www.ramp.studio/events/astrophd_tutorial/leaderboard). \n",
    "If there is an error (despite having tested your submission locally with `ramp_test_submission`), it will show up in the \"Failed submissions\" table in [my submissions](http://www.ramp.studio/events/astrophd_tutorial/my_submissions). You can click on the error to see part of the trace.\n",
    "\n",
    "After submission, do not forget to give credits to the previous submissions you reused or integrated into your submission.\n",
    "\n",
    "The data set we use at the backend is usually different from what you find in the starting kit, so the score may be different.\n",
    "\n",
    "The official score in this RAMP (the first score column after \"historical contributivity\" on the [leaderboard](http://www.ramp.studio/events/astrophd_tutorial/leaderboard)) is the IoU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contact\n",
    "\n",
    "Don't hesitate to ask your questions on the CDS Slack channel [#phd_tutorial](https://cds-upsay.slack.com/messages/CA25J6FLL) so the answers are shared with everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<footer style=\"float:right; color:#999;background:#fff;\">\n",
    "HAPPY CODING...\n",
    "</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "/*Mostly inspired from Lorena Barba's configuration\n",
       "https://github.com/barbagroup/AeroPython/blob/master/styles/custom.css*/\n",
       "\n",
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 750px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1100px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:600px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Alegreya Sans', sans-serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 200;\n",
       "    font-size: 50pt;\n",
       "    line-height: 100%;\n",
       "    /*color:#CD2305;*/\n",
       "    color:#62003C;\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Fenix', serif;\n",
       "    font-size: 22pt;\n",
       "    line-height: 100%;\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Fenix', serif;\n",
       "    margin-top:12px;\n",
       "    font-size: 16pt;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Fenix', serif;\n",
       "    font-size: 2pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Alegreya Sans', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 16pt;\n",
       "    color: #62003C;\n",
       "    font-style: italic;\n",
       "    margin-bottom: .5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'Source Code Pro', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       "figcaption {\n",
       "    /*display: block;*/\n",
       "    padding: 1.5em;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"style/custom.css\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
